{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practical3-RNN.ipynb","private_outputs":true,"provenance":[{"file_id":"1v8wVgXrlxCOP0FhZ8Nozo6vTgLd2dbri","timestamp":1644483744774}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"cLHNA01ChQM3"},"source":["<center><h1><b>Practical #3</b></h1>\n","<h2><b>RNNs: Recurrent Neural Networks</b><br/>\n","<b>for Text Generation</b></h2></center>\n","\n","RNNs are a type of neural networks which use the **output** they produce as **input** for the next element generated. This structure is generally very useful in generating any sequential data (such as text!). Check week 3's lectures for more details on how they work.\n","\n","<center><img width=600 src=\"https://drive.google.com/uc?id=1wotuCYP_0wRtOfGXc6xFhiEsPLXXqKpH\"></center>\n","\n","<br/>\n","\n","During today's practical you will:\n","1. Use given code to build the structure of a character-based RNN, load datasets and train it to observe results. \n","2. Scrape the web for more data to use in training the text generation model.\n","3. Follow-up with exercises exploring the possibilities for customising the RNN and ready-made interactive applications using RNNs.\n"]},{"cell_type":"markdown","metadata":{"id":"Axlwf5zNwOcq"},"source":["# Getting started"]},{"cell_type":"markdown","metadata":{"id":"j4oq4kNb0m03"},"source":["Create your own Jupyter notebook in Google Colab (or download a copy of this one so that you can edit it). Make sure that you enable GPU for the session (`Edit -> Notebook Settings -> Hardware accelerator -> GPU`). If you've made your own one, copy the code structure given here. Otherwise, just fill in the missing code.\n","\n","Next, import `tensorflow` into your project and check that everything is set up appropriately. The expected output from the following block of code is the name of the GPU (if nothing is printed after \"Found GPU at:\", then you don't have GPU access, please notify a demonstrator).\n","\n","We'll also perform the other required imports in this step."]},{"cell_type":"code","metadata":{"id":"ve7B5Sd52GVa"},"source":["#@title Imports\n","\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import os\n","import pandas as pd\n","\n","device_name = tf.test.gpu_device_name()\n","print('Found GPU at: {}'.format(device_name))\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XEkSB2qpx_ha"},"source":["# Data"]},{"cell_type":"markdown","metadata":{"id":"Lr5n6iS60tRl"},"source":["The first thing we'll do in today's session is to retrieve a data set of text. The RNN that we will train next on this data set will output similar text. You can experiment with different options (and even upload your own text file!), but for now we'll go with a book from [Project Gutenberg](https://www.gutenberg.org/ebooks/).\n","\n","Some other options:\n","*   **NLTK Text Corpora**: several datasets of various texts, which can be accessed through the `nltk` library. ([More info](https://www.nltk.org/book/ch02.html))\n","*   **Wikipedia**: a collection of cleaned Wikipedia articles in all languages. ([More info](https://www.tensorflow.org/datasets/catalog/wikipedia))\n","*   **Shakespeare**: \"The Winter's Tale\" by William Shakespeare ([link](https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt) - known to not be accessible at all times)\n","\n","---\n","\n","<br/>\n","\n","## <h1><img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-1:</b> Load a dataset</h1> \n","\n","<b>Choose</b> a book from Project Gutenberg, which will be loaded into a `text` variable. For any book on the website, if you navigate to its page, you can get to a link pointing to a plain-text version. Copy this link as the input for our dataset.\n","\n","<i>Optional</i>: check the code to understand how the dataset is loaded."]},{"cell_type":"code","metadata":{"id":"CLuwT55303Id"},"source":["# -*- coding: utf-8 -*-\n","#book_choice = \"https://www.gutenberg.org/files/1342/1342-0.txt\"  # @param {type: \"string\"}\n","#path_to_file = tf.keras.utils.get_file(\"Book\", book_choice)\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","artists = ['Led Zeppelin', 'The Doors', 'Chuck Berry']\n","df = pd.DataFrame()\n","for artist in artists:\n","    df = df.append(pd.read_csv(f'/content/gdrive/My Drive/Colab Notebooks/{artist}.csv'))\n","text = '\\n'.join(df['lyrics'])\n","#text = open(path_to_file, 'rb').read().decode(encoding='utf_8_sig')\n","\n","# The length of text is the number of characters in it\n","print (len(text))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OPiT5ipeLMQ3"},"source":["## <h1><img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-2:</b> Vectorize the text</h1> \n","\n","We need to vectorize the text (mapping all the characters to numbers), so that the neural network can process it (it works with numbers, not text!). In this example, we're going to be working at character level (i.e. generate sequences of characters).\n","\n","<b>Implement</b> the code to find the list of each unique character in the text (*hint*: a sorted set is useful). Store this in a variable called `vocabulary` and create 2 more data structures:\n","  * `char2idx`: a dictionary mapping from each unique character to its index in the list\n","  * `idx2char`: a numpy array of all unique characters in the text\n"]},{"cell_type":"code","metadata":{"id":"2H-SpzPPLXYf"},"source":["# TODO: compute the list of all unique characters in the file\n","vocabulary = sorted(set(text), key=lambda x:x)#TODO\n","# TODO: create the 2 data structures\n","char2idx = dict(zip(vocabulary, range(len(vocabulary))))\n","idx2char = np.array(vocabulary)#TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgLUR8Q9FqRH"},"source":["## Data pre-processing\n","\n","Let's keep track of some parameters related to our dataset. We recommend to keep `BATCH_SIZE` at 64, but you can play around with this."]},{"cell_type":"code","metadata":{"id":"ED8r7h_OBJ1G"},"source":["#@title Dataset parameters\n","\n","# batch size, default: 64\n","BATCH_SIZE = 64  # @param {type: \"integer\"}\n","# buffer size to shuffle our dataset, default 10000\n","BUFFER_SIZE = 10000  # @param {type: \"integer\"}\n","# number of RNN units, default 1024\n","N_RNN_UNITS = 1024  # @param {type: \"integer\"}\n","# length of text chunks for training, default 100\n","MAX_LENGTH =   40# @param {type: \"integer\"}\n","# size of the embedding layer, default 256\n","EMBEDDING_DIM = 256    # @param {type: \"integer\"}\n","\n","VOCAB_SIZE = len(vocabulary)  # length of the vocabulary in chars\n","print(\"Batch size: {} \\nBuffer size: {} \\n# RNN Units: {}\\\n","       \\nMax input length: {} \\nVocabulary size: {} \\nEmbedding dimension: {}\".format(\n","            BATCH_SIZE, BUFFER_SIZE, N_RNN_UNITS, MAX_LENGTH, VOCAB_SIZE, EMBEDDING_DIM\n","        )\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Rqmn6NR64nG"},"source":["Next we need to create the training data for the network. We want to be able to predict the next character in a sequence, and we will set this up as follows:\n","\n","1. Split the dataset text into chunks of size `MAX_LENGTH` set earlier, starting from the first character. This will be the input data.\n","1. Split the dataset text into chunks of size `MAX_LENGTH` set earlier, starting from the second character (thus including one more character than the input). This will be the target data.\n","1. Transform the chunks of text into number vectors, using the `char2idx` dictionary defined before.\n","\n","For example, take the dataset \"Tensorflow is great\". If `MAX_LENGTH` is set to 9, then we have input string \"tensorflo\" mapping to target output \"ensorflow\", and input \"w is grea\" mapping to target output \" is great\". This will efficiently teach the network how sequences of characters work, and what the probabilities of characters are based on `MAX_LENGTH` previous characters.\n"]},{"cell_type":"code","metadata":{"id":"HVt0Rx8uKXWF"},"source":["#@title Obtaining input and target data\n","\n","input_text = []\n","target_text = []\n","\n","for c in range(0, len(text)-MAX_LENGTH, MAX_LENGTH):\n","    inps = text[c : c + MAX_LENGTH]\n","    tars = text[c + 1 : c + 1 + MAX_LENGTH]\n","\n","    input_text.append([char2idx[i] for i in inps])\n","    target_text.append([char2idx[t] for t in tars])\n","    \n","print (np.array(input_text).shape)\n","print (np.array(target_text).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9mgokRadGEnN"},"source":["Next, we create batches from the data."]},{"cell_type":"code","metadata":{"id":"zuSRAsmmBLjV"},"source":["#@title Batch datasets\n","dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","dataset\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J37oVOWXA7m0"},"source":["And that's it! our data is all set up and ready to be fed into the RNN. "]},{"cell_type":"markdown","metadata":{"id":"to1H12deGZ9r"},"source":["# Build the network structure"]},{"cell_type":"markdown","metadata":{"id":"vcT3fzyMGex8"},"source":["Let's start setting up the network. The input for the generator is a vector of size `MAX_LENGTH`, with contents in the range [0, VOCAB_SIZE). The output is a probability distribution over the vocabulary available, indicating the probability for each character of appearing next in the sequence.\n","\n","---\n","We'll need the following layers for the network structure, and we use an [Adam optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam):\n","* **[Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)**: Turns positive integers into dense vectors of a fixed size.\n","* **[GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)**: Gated Recurrent Unit ([Cho et al. 2014](https://arxiv.org/pdf/1406.1078.pdf)), RNN network setup. GRU is a very similar architecture to LSTM (see lecture notes).\n","* **[Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)**: A densely connected layer.\n","\n","When you run the following code, it will output a summary of the network structure. We can also test this configuration with an example, and observe initial random output."]},{"cell_type":"code","metadata":{"id":"bRUbg3-_c2Hc"},"source":["#@title Set up generator network structure\n","\n","# Define the loss function\n","def loss_function(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","# Define input and output around the RNN (GRU)\n","def build_model(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, n_rnn_units=N_RNN_UNITS, batch_size=BATCH_SIZE):\n","    model = tf.keras.Sequential([\n","            tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                                      batch_input_shape=[batch_size, None]),\n","            tf.keras.layers.GRU(n_rnn_units,\n","                                return_sequences=True,\n","                                stateful=True,\n","                                recurrent_activation='sigmoid',\n","                                recurrent_initializer='glorot_uniform'),\n","            tf.keras.layers.Dense(vocab_size)\n","        ])\n","    model.summary()\n","    return model\n","\n","model = build_model()\n","\n","# Define the optimiser\n","# default: 0.001\n","opt_learning_rate = 0.001  #@param{type:\"raw\"}\n","# default: 0.5\n","opt_beta = 0.5 #@param{type:\"raw\"}\n","optimizer = tf.keras.optimizers.Adam(opt_learning_rate, beta_1=opt_beta)\n","\n","# Compile the model\n","model.compile(optimizer, loss_function)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bl_25oSl7Tzt"},"source":["#@title Test configuration with one example\n","\n","for input_example_batch, target_example_batch in dataset.take(1):\n","    # Run the batch through the model\n","    example_batch_predictions = model(input_example_batch)\n","\n","    # Print output shape\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","\n","    # To get the predictions, sample over the output distribution\n","    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy() \n","    \n","    # Decode the indices to see the text predicted by the (untrained) model\n","    print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])), \"\\n\")\n","    print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8hS1EUTtqJ3B"},"source":["# Train and run the network"]},{"cell_type":"markdown","metadata":{"id":"udNZyi_Yu1kK"},"source":["Once the model is trained (or even before!), we can run it! In this case, we want the model to generate some text (so, several characters, instead of just one), given some input by the user. Let's set up a function that does just that."]},{"cell_type":"code","metadata":{"id":"mg3_i32GUMJ9"},"source":["#@title Set up text generation function\n","\n","def generate_text(model, input_text, n_characters_output=1000):\n","    # First, vectorize the input text as before\n","    input_eval = [char2idx[s] for s in input_text]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    # We'll store results in this variable\n","    text_generated = []\n","\n","    # Generate the number of characters desired\n","    model.reset_states()\n","    for i in range(n_characters_output):\n","        # Run input through model\n","        predictions = model(input_eval)\n","\n","        # Remove the batch dimension\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        # Using a categorical distribution to predict the character returned by the model\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","        # Pass the predicted character as the next input to the model\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        # Add the predicted character to the output\n","        text_generated.append(idx2char[predicted_id])\n","\n","    # Return output\n","    return (input_text + ''.join(text_generated))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wjWQCZWlucqx"},"source":["In this practical, we'll make use of the built-in functions for training the model, as this network is much simpler than the GAN we explored before. First, we'll set up checkpoints at which the current state of the model should be saved, pointing to a directory in the GDrive. This will allow to restore the model at any point during training and use that to generate text or fine-tune from there."]},{"cell_type":"code","metadata":{"id":"Y5DRrVBA9KWE"},"source":["#@title Save checkpoints during training\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Directory where the checkpoints will be saved\n","path = 'My Drive/Work/Colab/TextGen/' #@param{type: 'string'}\n","full_path = \"/content/gdrive/\" + path + \"ckpt_{epoch}\" \n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","                      filepath=full_path,\n","                      save_weights_only=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-g0RQqbBex0q"},"source":["#@title Train the model\n","\n","# default: 50\n","n_epochs =  100# @param{type: \"integer\"} \n","history = model.fit(dataset, epochs=n_epochs, callbacks=[checkpoint_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAPiOhmICZqI"},"source":["#@title Restore latest checkpoint and build model\n","\n","batch_size = 1\n","\n","model = build_model(batch_size=batch_size)\n","model.load_weights(tf.train.latest_checkpoint(\"/content/gdrive/\" + path))\n","model.build(tf.TensorShape([batch_size, None]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zd3pj9H4EXxH"},"source":["#@title Generate text!\n","\n","#input_text = \"In the morning, \"  # @param {type: \"string\"}\n","\n","input_text = \"I love you\"  # @param {type: \"string\"}\n","n_characters_output = 1000 #@param \n","print(generate_text(model, input_text=input_text, n_characters_output=n_characters_output))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1mDE680qyxu"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"bb9X3LqfOto2"},"source":["## <h1><img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-3:</b> Acquiring more data (web scraping)</h1> "]},{"cell_type":"markdown","metadata":{"id":"nVX-IxPDN344"},"source":["When working with such text generation models, you might find you need more data - whether it is that data specific to your desired application is not readily available, or you simply need *more*. Let's look at how we can gather data which can then be used for training the same network we've set up here.\n","\n","The code in the next cell defines a way of scraping a [lyrics website](https://www.lyrics.com/) to find song lyrics from a particular artist. It uses as input the link to the artist's page, and then uses the HTML of that page to find links to lyrics of that artist's songs (with a maximum set so that the program doesn't take too long to run). \n","\n","The data obtained is then put into a `text` variable, from which point the usual code in the notebook for vectorizing the text, data-preprocessing and training the network can be run. Try it out! Is this network good at generating new songs for the chosen artist? "]},{"cell_type":"code","metadata":{"id":"f0lKYYAuRduz"},"source":["import requests\n","import lxml\n","from lxml import etree\n","from IPython import display\n","import time\n","\n","# Obtain the desired artist's page HTML\n","artist_url = 'https://www.lyrics.com/artist/Michael-Bubl%C3%A9/554516' #@param {type: \"string\"}\n","site_html = requests.get(artist_url)\n","\n","# Process this into a tree using lxml\n","html_tree = etree.HTML(site_html.content)\n","# html_text = str(etree.tostring(html_tree, pretty_print=True), \"utf-8\")\n","# print(html_text)\n","\n","# Extract data\n","# Albums and songs in \"<div class='tdata-ext'>\"\n","# Links are https://www.lyrics.com + x where \"<a href='x'>\"\n","# /album, /artist, or /lyric\n","\n","content_by_artist = html_tree.xpath('//div[contains(@class, \"tdata-ext\")]')[0]\n","# print(str(etree.tostring(div, pretty_print=True), \"utf-8\"))\n","content_list = content_by_artist.xpath('//a/@href')\n","filter_list = [x for x in content_list if x.startswith('/lyric')]\n","# print(filter_list)\n","print(len(filter_list))\n","\n","max_songs = 20 #@param{type: 'number'}\n","data = []\n","s = 0\n","for lyric_url in filter_list:\n","    url = 'https://www.lyrics.com' + lyric_url\n","    html = requests.get(url)\n","    tree = etree.HTML(html.content)\n","    lyric_html = tree.xpath('//pre[contains(@class, \"lyric-body\")]')\n","    if len(lyric_html) == 1:\n","        lyric_html = lyric_html[0]\n","    else:\n","        continue\n","    # print(str(etree.tostring(lyric_html, pretty_print=True), \"utf-8\"))\n","\n","    lyrics = ''.join(lyric_html.xpath('.//text()'))\n","    data.append(lyrics)\n","\n","    s += 1\n","    display.clear_output()\n","    print(\"Processed:\",s)\n","    if s > max_songs:\n","        break\n","    if s % 10 == 0:\n","        time.sleep(0.5)\n","\n","# Save all lyrics for the artist in a single string called 'text'\n","text = '\\n'.join(data)\n","\n","# Print out some of the text obtained\n","display.clear_output()\n","print(text[:500])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVeJYhD5FVKV"},"source":["\n","1. <b>Repeat</b> the whole process for a different artist of your choice (*note*: you may need to adapt the html processing code, depending on the web page you're using!)."]},{"cell_type":"markdown","metadata":{"id":"Uw3idx4uq619"},"source":["## <img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-4:</b> Customize the network\n","\n","1. You will find several **parameters** in this notebook, highlighted on the right side of the code blocks (e.g. learning rate for the optimiser). If you modify some of these values, how do you think it will impact the quality of the text generated? \n","  <ol type=\"a\">\n","  <li>Is it better, or worse? </li>\n","  <li>Does it need more or less training time to start generating good text?  </li>\n","  <li>Try some different values and check if your intuition is correct.</li>\n","  </ol>\n","\n","2. The **structure** of the network can also be modified.  \n","\n","  <ol type=\"a\">\n","  <li>Look into the different activation functions available and check if others work better or worse in this context.</li>\n","  <li>What other optimisers could be used for better performance?</li>\n","  <li>What about the layers? What happens if another RNN layer is added to the network?</li>\n","  </ol>"]},{"cell_type":"markdown","metadata":{"id":"Biu-4xvOcHpc"},"source":["## <img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-5:</b> Test online RNN applications\n","\n","1. [Oleksii Trekhleb's Machine Learning Experiments](https://trekhleb.github.io/machine-learning-experiments/#/) offers several browser-based software, allowing you to run RNNs to generate Shakespeare, Wikipedia-like text, recipes or sum numbers. Scroll to the bottom of the page to find the RNN applications.\n","1. [AI Dungeon](https://play.aidungeon.io/) uses GPT-2 to create text-based adventures. Explore this game and try to check what happens if you fight a dragon with a candlestick.\n","1. [Write With Transformer](https://transformer.huggingface.co/doc/gpt2-large) is a web application using GPT2 for text prediction while writing a document.\n","1. [Talk to Transformer](https://app.inferkit.com/demo) is a text generation demo working similarly to today's practical, using GPT-2.\n"]},{"cell_type":"markdown","metadata":{"id":"pIrtovFTVa2l"},"source":["## Related tutorials\n","\n","Some of the materials in this practical were based on the following tutorials:\n","\n","* Tensorflow, [Text Generation](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb)\n","* Oleksii Trekhleb, [Wikipedia Text Generation](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb)\n","* Max Woolf, [textgenrnn](https://colab.research.google.com/drive/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK)\n","\n","## GPT-2 fine-tuning tutorial\n","\n","The powerful GPT-2 model is available for download and public use. The Generative Pre-trained Transformer (GPT) 2 is a large model developed by OpenAI and trained over a large collection of data for a long time (thus using more resources than regularly available). If you'd like to read more about this, please refer to [this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and the [OpenAI blog](https://openai.com/blog/better-language-models/).\n","\n","[This tutorial](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) provides a nice introduction to downloading an easy to use version of the GPT-2 model (in different sizes as well), although it is only compatible with Tensorflow 1.x at the moment of writing (in Google Colab, you can use the command `%tensorflow_version 1.x` **before** you import tensorflow to change versions)."]}]}